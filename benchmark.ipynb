{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tellg\\Documents\\Universita\\VISIOPE\\2023-2024\\Progetto\\ContinualLearning\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from avalanche.benchmarks import SplitMNIST\n",
    "from avalanche.training.supervised import EWC\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.logging import TensorboardLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics, timing_metrics, forgetting_metrics\n",
    "from avalanche.benchmarks.classic import CORe50\n",
    "from avalanche.logging import InteractiveLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels...\n",
      "Loading LUP...\n",
      "Loading labels names...\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "benchmark = CORe50(scenario=\"nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Definizione delle classi che vogliamo includere\n",
    "target_classes = list(range(10))  # Utilizziamo solo le prime 10 classi\n",
    "\n",
    "# Funzione per filtrare le classi desiderate\n",
    "def filter_classes(dataset, target_classes):\n",
    "    targets = np.array(dataset.targets)\n",
    "    mask = np.isin(targets, target_classes)\n",
    "    indices = np.where(mask)[0]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# Creazione del train e test stream con classi filtrate\n",
    "train_datasets = [filter_classes(exp.dataset, target_classes) for exp in benchmark.train_stream]\n",
    "test_datasets = [filter_classes(exp.dataset, target_classes) for exp in benchmark.test_stream]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creazione del logger di TensorBoard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# Plugin per la valutazione\n",
    "eval_plugin = EvaluationPlugin(    \n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\\\n",
    "    loggers=[tb_logger])\n",
    "\"\"\"\n",
    "# Definizione del plugin di valutazione con TensorBoard\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[tb_logger]\n",
    ")\n",
    "\"\"\"\n",
    "# Lista dei modelli\n",
    "models = [SimpleCNN(num_classes=len(target_classes))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: SimpleCNN\n",
      "Training on dataset with 23980 samples\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Subset' object has no attribute 'origin_stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_dataset, test_dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(train_datasets, test_datasets):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     \u001b[43mcl_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating on test dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tellg\\Documents\\Universita\\VISIOPE\\2023-2024\\Progetto\\ContinualLearning\\.venv\\lib\\site-packages\\avalanche\\training\\templates\\base_sgd.py:211\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[1;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    205\u001b[0m     experiences: Union[TDatasetExperience, Iterable[TDatasetExperience]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    210\u001b[0m ):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(experiences, eval_streams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mget_last_metrics()\n",
      "File \u001b[1;32mc:\\Users\\tellg\\Documents\\Universita\\VISIOPE\\2023-2024\\Progetto\\ContinualLearning\\.venv\\lib\\site-packages\\avalanche\\training\\templates\\base.py:157\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[1;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_streams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     eval_streams \u001b[38;5;241m=\u001b[39m [experiences_list]\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_streams \u001b[38;5;241m=\u001b[39m \u001b[43m_group_experiences_by_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_training(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience \u001b[38;5;129;01min\u001b[39;00m experiences_list:\n",
      "File \u001b[1;32mc:\\Users\\tellg\\Documents\\Universita\\VISIOPE\\2023-2024\\Progetto\\ContinualLearning\\.venv\\lib\\site-packages\\avalanche\\training\\templates\\base.py:357\u001b[0m, in \u001b[0;36m_group_experiences_by_stream\u001b[1;34m(eval_streams)\u001b[0m\n\u001b[0;32m    355\u001b[0m exps_by_stream \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m exps:\n\u001b[1;32m--> 357\u001b[0m     sname \u001b[38;5;241m=\u001b[39m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morigin_stream\u001b[49m\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    358\u001b[0m     exps_by_stream[sname]\u001b[38;5;241m.\u001b[39mappend(exp)\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# Finally, we return a list of lists.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Subset' object has no attribute 'origin_stream'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Iterazione sui modelli e allenamento\n",
    "for model in models:\n",
    "    \n",
    "    print(f\"Training model: {model.__class__.__name__}\")\n",
    "\n",
    "    # Definizione della strategia EWC\n",
    "    cl_strategy = EWC(\n",
    "        model=model,\n",
    "        optimizer=torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n",
    "        criterion=torch.nn.CrossEntropyLoss(),\n",
    "        ewc_lambda=0.4,  # lambda per regolarizzazione EWC\n",
    "        train_mb_size=32,\n",
    "        train_epochs=1,\n",
    "        eval_mb_size=100,\n",
    "        device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        evaluator=eval_plugin\n",
    "    )\n",
    "    # Esecuzione dell'allenamento e valutazione con i dataset filtrati\n",
    "    for train_dataset, test_dataset in zip(train_datasets, test_datasets):\n",
    "        print(f\"Training on dataset with {len(train_dataset)} samples\")\n",
    "        cl_strategy.train(train_dataset)\n",
    "        print(\"Training completed\")\n",
    "        \n",
    "        print(\"Evaluating on test dataset\")\n",
    "        cl_strategy.eval(test_dataset)\n",
    "        print(\"Evaluation completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
